
## 요구사항

- 아래 API 목록을 구현 합니다. 
  - 광고 등록 API 
  - 광고 조회 API 
  - 광고 참여 API 
  - 광고 참여 이력 조회 API 
- 어플리케이션이 다수의 서버에 다수의 인스턴스로 동작하더라도 문제가 없어야 합니다. 
- 각 기능 및 제약사항에 대한 단위테스트를 반드시 작성합니다.

## 평가항목

- 프로젝트 구성 방법 및 관련된 시스템 아키텍쳐 설계 방법이 적절한가? 
- 요구사항을 잘 이해하고 구현하였는가? 
- 작성한 어플리케이션 코드의 가독성 좋고 의도가 명확한가? 
- 작성한 테스트 코드는 적절한 범위의 테스트를 수행하고 있는가? (예. 유닛/통합 테스트 등)
- 어플리케이션은 다량의 트래픽에도 무리가 없도록 효율적으로 작성되었는가?


## 작성사항

- 설계 내용과 설계의 이유
- 핵심 문제해결 전략 및 분석한 내용

## 시스템 설계를 위한 대략적인 계산 (Back-of-the-envelope Estimation)

---
데이터 근거: ${프로젝트경로}/kakaopay_AD_product_2406.pdf

**Users**

- 사용자 구분: 일반 사용자 / 광고 가맹점
  - 일반 사용자: 광고 조회, 광고 참여, 광고
  - 광고 가맹점: 광고 등록
- 전체 수: 4,000만명 / 100만개
- MAU: 2,400만명 / 50만개 (높은 활성 비율 50% 기준) 
- DAU
  - 일반 사용자: 디스플레이 전체 광고 상품 단가표 기준(페이홈>홈 기준) 
    - 아이콘(고정형)/아이콘(확장형)/전면팝업/머니박스 상단/브랜딩보드
    - (1,500만 + 1,500만 + 150만 + 800만 + 1,500만) / 7(주 단위) 
    - = 7,785,714 ≈ 800만
  - 광고 가맹점: 높은 활성 비율(20% 기준)
    - 50만 * 20% = 10만

**TPS(Transaction Per Second)**

- 하루 초: 86,400초 
- 일반 사용자
  - 사용자는 하루에 평균 페이지 방문 횟수: 2회 (카카오페이 화면 방문 가정 수치)
  - 페이지 내 광고 상호작용 3(광고 조회, 광고 참여, 광고 참여 조회 이력) 
  - TPS = DAU / 86,400 * 2(평균 페이지 방문 횟수) * 3(광고 상호작용)
    - 800만 / 86,400 * 2 * 3 
    - = 556 ≈ 600 TPS 
- 광고 가맹점
  - 광고 가맹점은 하루에 평균 페이지 방문 횟수: 1회 
  - 광고 상호작용 1(광고 등록)
  - TPS = DAU / 86,400 * 1(평균 페이지 방문 횟수) * 1(광고 상호작용)
  - 10만 / 86,400 * 1 * 1
  - = 1.16 ≈ 1 TPS

**QPS(Query Per Second)**

- 한 광고 트랜잭션당 2번의 쿼리가 발생한다고 가정(공통)
- 총 쿼리수는 TPS * 쿼리 수
- 일반 사용자
  - QPS = TPS * 2
    - 600 TPS * 2 = 1,200 QPS
  - Peak QPS = QPS * 2
    - 1,200 QPS * 2 = 2,400 QPS
- 광고 가맹점
  - QPS = TPS * 2
    - 1 TPS * 2 = 2 QPS
  - Peak QPS = QPS * 2
    - 2 QPS * 2 = 4 QPS

**Storage(DB)**

- 광고, 광고 참여 이력(부수적인 데이터 무시)
  - 광고
    - Size: 500 bytes(가정)
    - 등록 빈도수: 가맹점 TPS = 1 TPS  
      - day ≈ 86,400 rows / 40 MB
      - year ≈ 3,200만 rows / 1400 TB
  - 광고 참여 이력
    - Size: 100 bytes(가정)
    - 등록 빈도수: 일반 사용자 TPS / 3 = 200 TPS
      - day ≈ 1,700만 rows / 2 GB
      - year ≈ 63억 rows / 11,000 PB


**정리** 

| 구분            | 사용자       | 가맹점      |
|---------------|-----------|----------|
| 전체 수          | 4,000만명   | 100만개    |
| MAU           | 2,400만명   | 50만개     |
| DAU           | 800만명     | 10만개     |
| TPS           | 600 TPS   | 1 TPS    |
| QPS           | 1,200 QPS | 2 QPS    |
| Peak QPS      | 2,400 QPS | 4 QPS    |
| ROW(Day)      | 1,700만    | 86,400개  |
| ROW(Year)     | 63억       | 3,200만개  |
| STORAGE(Day)  | 2 GB      | 40 MB    |
| STORAGE(Year) | 11,000 PB | 1,400 TB |


## 시스템 아키텍처

---
![ads_archi](https://github.com/user-attachments/assets/733d9fe3-455c-4616-a1b4-26c19a57b009)
- 사전 과제 미포함 요소: API Gateway, Load Balancer, CDN
- 사전 과제 구성 변경 요소: API Server, Consumer 서버는 하나의 프로젝트로 구동

### 설계 고려 사항

1. 캐시 계층 도입: 광고 조회와 같은 빈번한 데이터 조회 API에 대한 응답 시간을 줄이기 위해 캐싱 활용
2. 서비스 별 데이터베이스 구분
   3. 광고 서비스: 사용자 대비 적은 가맹점 데이터, 참여 횟수, 기간 등 데이터 일관성 유지 필요하여 ACID 트랜잭션 지원하는 RDBMS 활용  
   4. 사용자/광고 참여 서비스: 대량의 트래픽, 데이터를 고려하여, 대용량 데이터 및 수평적 확장성을 고려한 NoSQL
3. 메시지 큐 및 비동기 처리
   4. 여러 사용자가 동시에 광고에 참여할 경우, 락을 사용자 서비스에서 적용하면 서비스 지연 발생.
   5. 광고 참여 발생 이벤트를 큐에 넣고, 이후 처리를 비동기로 수행하여 API 응답 및 서비스 가용성 개선

### 설계 미포함 고려 사항
대용량 데이터
- 조회를 위한 인덱스는 어떻게 ?
  - 동적 파티셔닝 ?
- 저장 공간은 어떻게 ? (사용자 1년: 63억 row / 11,000PB , 가맹점 1년: 3,200만개 / 1,400 TB)
  - 파티셔닝? 동적 테이블?


대량의 트래픽을 감당하면서도 높은 성능과 안정성을 유지하기 위해서는 확장 가능하고 견고한 아키텍처를 설계하는 것이 중요합니다. 주어진 API 요구사항을 바탕으로 한 서비스 아키텍처는 **고가용성**, **확장성**, **내결함성**, 그리고 **데이터 일관성**을 고려하여 설계될 수 있습니다. 아래는 이 목표를 달성하기 위한 시스템 디자인 예시입니다.

### 1. **API Gateway**
- **역할**: 모든 API 요청을 처리하는 진입점으로, 인증, 트래픽 분배, 로깅, 모니터링을 담당합니다.
- **기능**:
  - API 요청을 처리하기 전에 인증 및 권한 검사를 수행.
  - **Rate Limiting**(요청 속도 제한)을 적용하여 트래픽 폭주를 방지.
  - **로드 밸런서**를 통해 백엔드 서비스에 균등하게 트래픽을 분배.

### 2. **로드 밸런싱**
- **역할**: API 서버로 가는 트래픽을 분산하여 시스템 과부하를 방지합니다.
- **종류**:
  - **HTTP 로드 밸런서**: HTTP 요청을 여러 서버에 분산.
  - **Elastic Load Balancer (ELB)** 또는 **NGINX** 같은 소프트웨어 솔루션을 사용할 수 있습니다.
- **기능**: 서버 다운타임이나 과부하 시 자동으로 새로운 서버로 트래픽을 전환.

### 3. **API 서버**
- **역할**: 비즈니스 로직을 처리하는 서비스입니다. 각 API는 이 계층에서 처리됩니다.
- **설계**: **Stateless** 아키텍처로 설계하여 서버 간의 상태를 공유하지 않으며, 이를 통해 쉽게 수평적 확장이 가능합니다.
- **스케일링**: **오토 스케일링**을 적용하여 트래픽에 따라 동적으로 서버 인스턴스를 늘리거나 줄일 수 있습니다.

### 4. **캐시 계층**
- **역할**: 광고 조회와 같은 빈번한 데이터 조회 API에 대한 응답 시간을 줄이기 위해 캐싱을 활용.
- **Redis** 또는 **Memcached**와 같은 **인메모리 캐시** 솔루션을 사용하여 광고 조회 API의 결과를 캐싱.
- **TTL(Time To Live)**을 설정하여 특정 기간 동안만 캐시된 데이터를 유지.

### 5. **데이터베이스 계층**
- **역할**: 광고 등록, 조회, 참여, 참여 이력과 같은 데이터를 저장하고 관리합니다.
- **데이터베이스 선택**:
  - **RDBMS**: 광고 참여 이력과 같은 구조화된 데이터를 저장하는 데 적합하며, **MySQL**, **PostgreSQL** 등을 사용할 수 있습니다.
  - **NoSQL**: 광고 조회와 같은 트래픽이 많은 데이터는 **Cassandra**나 **MongoDB** 같은 **NoSQL** DB로 처리 가능.
- **트랜잭션 처리**: 데이터 일관성을 유지하기 위해 트랜잭션 처리를 지원하는 데이터베이스를 사용하며, 필요에 따라 **ACID** 트랜잭션을 지원하는 RDBMS를 활용.

### 6. **광고 참여 가능 횟수 처리** (동시성 관리)
- **문제**: 여러 사용자가 동시에 광고에 참여할 경우, 광고 참여 가능 횟수가 정확하게 관리되어야 함.
- **해결**: **분산 락** 또는 **낙관적 락**을 적용해 동시에 여러 사용자가 광고에 참여할 때도 정확한 카운팅이 가능하도록 구현.
  - **Redis** 기반의 **분산 락**이나 데이터베이스 레벨에서의 **낙관적 잠금**을 통해 데이터 무결성을 보장.
  - **참여 횟수 감소**를 위한 비동기 **큐 기반** 아키텍처를 도입하여 광고 참여 처리를 효율적으로 처리.

### 7. **메시지 큐 및 비동기 처리**
- **역할**: 광고 참여 API와 같은 중요한 요청의 비동기 처리 및 지연 없는 트랜잭션 처리를 위해 큐 시스템을 도입.
- **RabbitMQ**, **Apache Kafka**와 같은 **메시지 브로커**를 사용하여 광고 참여 시 발생하는 이벤트를 큐에 넣고, 이후 처리를 비동기로 수행합니다.
  - **적립 API** 호출과 같은 외부 서비스와의 연동 작업을 비동기적으로 처리하여 API 응답 속도를 개선.

### 8. **광고 참여 이력 저장 및 조회**
- **역할**: 대량의 데이터가 누적될 수 있으므로, 광고 참여 이력 조회는 효율적으로 처리해야 함.
- **데이터 파티셔닝**: 대용량 데이터를 처리하기 위해 **수평 파티셔닝**을 적용하여 여러 DB 인스턴스에 데이터를 분산.
- **페이징 처리**: API에서 **페이지네이션**을 적용하여 요청당 많은 데이터를 처리하지 않도록 구현.

### 9. **모니터링 및 로깅**
- **역할**: 시스템 상태를 실시간으로 모니터링하고 문제가 발생할 경우 즉각적으로 대응할 수 있도록 설계.
- **Prometheus**와 **Grafana**를 사용하여 **TPS**, **QPS**, 오류율, 서버 상태 등을 모니터링.
- **ELK 스택**(Elasticsearch, Logstash, Kibana)을 사용하여 로그를 수집, 분석, 시각화.

### 10. **CDN(Content Delivery Network)**
- **역할**: 광고 이미지와 같은 정적 자원은 **CDN**을 통해 전 세계 사용자에게 빠르게 제공.
- **Cloudflare** 또는 **AWS CloudFront**와 같은 CDN을 통해 광고 이미지 URL이 빠르게 로드되도록 구성.

---

### **예시 아키텍처 다이어그램**

```
User -> API Gateway -> Load Balancer -> API Servers
                      |               |
                      |               -> Cache Layer (Redis/Memcached)
                      |               
                      -> Message Queue (Kafka/RabbitMQ) 
                      -> Databases (RDBMS/MySQL, NoSQL)
                      -> CDN (광고 이미지 제공)
```

### **TPS/QPS를 고려한 시스템 확장**
- **오토 스케일링**을 통해 API 서버와 데이터베이스를 동적으로 확장.
- **캐시 시스템**을 사용하여 빈번한 조회 요청을 효율적으로 처리.
- **큐 시스템**을 통해 높은 트래픽을 분산 처리.
- 트래픽 증가 시에도 무리 없는 성능을 유지할 수 있도록 **분산 데이터베이스**, **분산 락** 및 **메시지 큐** 기반 아키텍처를 활용.

이 설계는 대규모 트래픽과 동시성 처리를 견딜 수 있는 견고한 서비스 구조를 제공합니다.


## 실제 시스템 디자인

- API 서버
  - 비즈니스 로직 처리, Stateless 아키텍처로 설계하여 서버 간의 상태를 공유하지 않음 -> 이를 통해 쉽게 수평적 확장이 가능
  - 스케일링: 오토 스케일링을 적용하여 트래픽에 따라 동적으로 서버 인스턴스를 늘리거나 줄일 수 있음
- 캐시 계층
  - 광고 조회와 같은 빈번한 데이터 조회 API에 대한 응답 시간을 줄이기 위해 캐싱을 활용
  - Redis로 광고 조회 API의 결과를 캐싱
  - TTL 을 설정하여 특정 기간 동안만 캐시된 데이터를 유지
- 데이터베이스 계층
  - 광고 등록, 조회, 참여, 참여 이력과 같은 데이터 저장 및 관리
  - RDBMS: 광고 참여 이력과 같은 구조화된 데이터를 저장하는데 적합, MySQL
  - NoSQL: 광고 조회와 같은 트래픽이 많은 데이터는 MongoDB 같은 NoSQL로 처리
  - 트랜잭션 처리: 데이터 일관성을 유지하기 위해 ACID 트랜잭션을 지원하는 RDBMS를 활용
- 광고 참여 가능 횟수 처리 (동시성 관리)
  - Redis 기반의 분산 락을 적용해 동시에 여러 사용자가 광고에 참여할 때도 정확한 카운팅이 가능하도록 구현
  - 참여 횟수 감소를 위한 비동기 큐 기반 아키텍처 도입
- 메시지 큐 및 비동기 처리
  - 광고 참여 API와 같은 중요한 요청의 비동기 처리 및 지연 없는 트랜잭션 처리를 위해 큐 시스템 도입
- 광고 참여 이력 저장 및 조회
  - 대량의 데이터가 누적될 수 있으므로, 광고 참여 이력 조회는 효율적으로 처리
  - 데이터 파티셔닝: 대용량 데이터를 처리하기 위해 수평 파티셔닝 적용하여 여러 DB 인스턴스에 데이터를 분산
  - 페이징 처리: API에서 페이지네이션을 적용
- 